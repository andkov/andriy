<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Recent &amp; Upcoming Talks on Andriy Koval</title>
    <link>/talk/</link>
    <description>Recent content in Recent &amp; Upcoming Talks on Andriy Koval</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019 Andriy Koval</copyright>
    <lastBuildDate>Wed, 06 Nov 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/talk/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Implementing Reproducible Visualizations</title>
      <link>/talk/2019-11-08-visualizing-logistic-regression/</link>
      <pubDate>Wed, 06 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/talk/2019-11-08-visualizing-logistic-regression/</guid>
      <description>Visualising results of statistical modeling is a key component of data science workflow. Statistical graphs often is the best means to explain and promote research findings. However,in order to find that one graph that tells the story worth sharing, we sometimes have to try out and sift through many data visualizations. How should we approach such a task? What can we do to make it easier from both production and evaluation perspectives?</description>
    </item>
    
    <item>
      <title>Visualizing Logistic Regression</title>
      <link>/talk/2018-11-01-visualizing-logistic-regression/</link>
      <pubDate>Thu, 01 Nov 2018 16:00:00 -0700</pubDate>
      
      <guid>/talk/2018-11-01-visualizing-logistic-regression/</guid>
      <description>Visualising results of statistical modeling is a key component of data science workflow. Statistical graphs are often the best means to explain and promote research findings. However, in order to find that one graph that tells the story worth sharing, we sometimes have to try out and sift through many data visualizations. How should we approach such a task? What can we do to make it easier from both production and evaluation perspectives?</description>
    </item>
    
    <item>
      <title>When notebooks are not enough</title>
      <link>/talk/2018-10-31-when-notebooks-are-not-enough/</link>
      <pubDate>Wed, 31 Oct 2018 15:00:00 -0700</pubDate>
      
      <guid>/talk/2018-10-31-when-notebooks-are-not-enough/</guid>
      <description>AbstractWhile computational notebooks offer scientists and engineers many helpful features, the limitations of this medium make it but a starting point in creating software - the practical goal of data science. Where do we go from computational notebooks if our projects require multiple interconnected scripts and dynamic documents? How do we ensure reproducibility amidst growing complexity of analyses and operations?
I will use a concrete analytical example to demonstrate how constructing workflows for reproducible analyses can serve as the next step from computational notebooks towards creating an analytical software.</description>
    </item>
    
    <item>
      <title>Toolbox and Toolsets of Reproducible Research</title>
      <link>/talk/2014-10-10-toolbox-toolset-reproducible-research/</link>
      <pubDate>Fri, 10 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/talk/2014-10-10-toolbox-toolset-reproducible-research/</guid>
      <description>The lecture introduces reproducible research and demonstrates digital self-publishing with RStudio and Git (Hub). The skills described and emphasized in this workflow include data manipulation, graph production, statistical modeling, and dynamic reporting. A series of four talks discusses each skill and gives examples of possible implementations in R.</description>
    </item>
    
  </channel>
</rss>