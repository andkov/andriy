<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ggplot2 on Andriy Koval</title>
    <link>/tags/ggplot2/</link>
    <description>Recent content in ggplot2 on Andriy Koval</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019 Andriy Koval</copyright>
    <lastBuildDate>Wed, 06 Nov 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/ggplot2/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Implementing Reproducible Visualizations</title>
      <link>/talk/2019-11-08-visualizing-logistic-regression/</link>
      <pubDate>Wed, 06 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/talk/2019-11-08-visualizing-logistic-regression/</guid>
      <description>Visualising results of statistical modeling is a key component of data science workflow. Statistical graphs often is the best means to explain and promote research findings. However,in order to find that one graph that tells the story worth sharing, we sometimes have to try out and sift through many data visualizations. How should we approach such a task? What can we do to make it easier from both production and evaluation perspectives?</description>
    </item>
    
    <item>
      <title>Suppressing Small Counts for Public Release</title>
      <link>/publication/koval-2019-suppress-for-release/</link>
      <pubDate>Wed, 29 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/koval-2019-suppress-for-release/</guid>
      <description>Demonstrates the methods of suppressing small counts in a provincial surveillance system in preparation of data for public release.</description>
    </item>
    
    <item>
      <title>Visualizing Logistic Regression</title>
      <link>/talk/2018-11-01-visualizing-logistic-regression/</link>
      <pubDate>Thu, 01 Nov 2018 16:00:00 -0700</pubDate>
      
      <guid>/talk/2018-11-01-visualizing-logistic-regression/</guid>
      <description>Visualising results of statistical modeling is a key component of data science workflow. Statistical graphs are often the best means to explain and promote research findings. However, in order to find that one graph that tells the story worth sharing, we sometimes have to try out and sift through many data visualizations. How should we approach such a task? What can we do to make it easier from both production and evaluation perspectives?</description>
    </item>
    
    <item>
      <title>When notebooks are not enough</title>
      <link>/talk/2018-10-31-when-notebooks-are-not-enough/</link>
      <pubDate>Wed, 31 Oct 2018 15:00:00 -0700</pubDate>
      
      <guid>/talk/2018-10-31-when-notebooks-are-not-enough/</guid>
      <description>AbstractWhile computational notebooks offer scientists and engineers many helpful features, the limitations of this medium make it but a starting point in creating software - the practical goal of data science. Where do we go from computational notebooks if our projects require multiple interconnected scripts and dynamic documents? How do we ensure reproducibility amidst growing complexity of analyses and operations?
I will use a concrete analytical example to demonstrate how constructing workflows for reproducible analyses can serve as the next step from computational notebooks towards creating an analytical software.</description>
    </item>
    
  </channel>
</rss>