<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Andriy Koval</title>
    <link>/</link>
    <description>Recent content on Andriy Koval</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019 Andriy Koval</copyright>
    <lastBuildDate>Fri, 27 Mar 2020 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Florida Demographic Growth</title>
      <link>/post/2020-03-27-florida-demographic-growth/</link>
      <pubDate>Fri, 27 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/2020-03-27-florida-demographic-growth/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;!-- These two chunks should be added in the beginning of every .Rmd that you want to source an .R script --&gt;
&lt;!--  The 1st mandatory chunck  --&gt;
&lt;!--  Set the working directory to the repository&#39;s base directory --&gt;
&lt;!--  The 2nd mandatory chunck  --&gt;
&lt;!-- Set the report-wide options, and point to the external code file. --&gt;
&lt;div id=&#34;abstract&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Abstract&lt;/h1&gt;
&lt;p&gt;This blogposts shows how to extract population estimates data reported by the Florida Department of Health, prepare them for analysis, and conduct basic exploration of the demographic growth.&lt;/p&gt;
&lt;p&gt;See the report in its native environment of &lt;a href=&#34;https://github.com/dss-hmi/suicide-prevention-2019&#34;&gt;suicide-prevention-2019&lt;/a&gt; repository &lt;a href=&#34;https://raw.githack.com/dss-hmi/suicide-prevention-2019/3ff78365931214e342640523f2096c2eac443b2f/analysis/blogposts/florida-demographic-growth/fl-demo-growth.html&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;environment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Environment&lt;/h1&gt;
&lt;!-- Load &#39;sourced&#39; R files.  Suppress the output when loading packages. --&gt;
&lt;pre class=&#34;r numberLines&#34;&gt;&lt;code&gt;# Attach these packages so their functions don&amp;#39;t need to be qualified
# see http://r-pkgs.had.co.nz/namespace.html#search-path
library(magrittr) #Pipes
library(ggplot2)  # graphs
library(dplyr)
requireNamespace(&amp;quot;tidyr&amp;quot;)    # data tidying
requireNamespace(&amp;quot;ggpubr&amp;quot;)   # publication plots
requireNamespace(&amp;quot;readxl&amp;quot;)   # data import&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-wrangling&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data Wrangling&lt;/h1&gt;
&lt;div id=&#34;data-origin&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Origin&lt;/h2&gt;
&lt;p&gt;The initial extract of the data was obtained from &lt;a href=&#34;http://www.flhealthcharts.com/FLQUERY/Population/PopulationRpt.aspx&#34;&gt;www.flhealthcharts.com&lt;/a&gt; a reporting tool for population counts estimated by the Florida Department of Health. The figure below shows the modifications to the default query the produces the data product used in this demonstration:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;data/FloridaPopulation-2006-2020.PNG&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;View of the reporting tool&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The tool gives the option to save the product of the query as an Excel book (&lt;code&gt;.xls&lt;/code&gt;), however, the import of this extension into R has been problematic, so I have converted (“save as”) the file manually into a more modern Excel format, &lt;code&gt;.xlsx&lt;/code&gt;. This file is the raw source for the current report and can be dowloaded for closer inspection &lt;a href=&#34;https://github.com/dss-hmi/suicide-prevention-2019/raw/3ff78365931214e342640523f2096c2eac443b2f/analysis/blogposts/florida-demographic-growth/data/FloridaPopulation.xlsx&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-import&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data import&lt;/h2&gt;
&lt;p&gt;The structure of the Excel file requires some tidying to enable a nimble analytic flow&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;data/extracted-data.PNG&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;View of the extracted data&lt;/p&gt;
&lt;/div&gt;
&lt;!-- Load any Global functions and variables declared in the R file.  Suppress the output. --&gt;
&lt;pre class=&#34;r numberLines&#34;&gt;&lt;code&gt;# you will need to replace this path to the location where you stored your data file
path_file_input &amp;lt;- &amp;quot;./content/post/2020-03-27-florida-demographic-growth/data/FloridaPopulation.xlsx&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;!-- Declare any global functions specific to a Rmd output.  Suppress the output. --&gt;
&lt;!-- # ```{r, echo=echoChunks, message=FALSE} --&gt;
&lt;!-- # #Put code in here.  It doesn&#39;t call a chunk in the codebehind file. --&gt;
&lt;!-- # ``` --&gt;
&lt;!-- Load the datasets.   --&gt;
&lt;pre class=&#34;r numberLines&#34;&gt;&lt;code&gt;ds0 &amp;lt;-  readxl::read_excel(path_file_input, col_names = FALSE, skip = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-tweaking&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Tweaking&lt;/h2&gt;
&lt;p&gt;We can identify several problems to address:&lt;br /&gt;
1. Not all columns have names&lt;br /&gt;
2. &lt;code&gt;race&lt;/code&gt;, &lt;code&gt;ethnicity&lt;/code&gt;, &lt;code&gt;sex&lt;/code&gt;, and &lt;code&gt;age_group&lt;/code&gt; are stored in merged spreadsheet cells&lt;br /&gt;
3. Sums for categories are recorded in rows as observations (e.g &lt;code&gt;Total&lt;/code&gt;)&lt;br /&gt;
4. Some values of &lt;code&gt;age_group&lt;/code&gt; are misinterpreted as dates (e.g. &lt;code&gt;1-4&lt;/code&gt; becomes &lt;code&gt;4-Jan&lt;/code&gt;)&lt;br /&gt;
5. &lt;code&gt;age_group&lt;/code&gt; does not bin the population evenly (e.g. &lt;code&gt;20-24&lt;/code&gt; vs &lt;code&gt;25-34&lt;/code&gt;)&lt;/p&gt;
&lt;div id=&#34;tweak-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tweak 1&lt;/h3&gt;
&lt;p&gt;To address (1) we skipped the first 3 lines during import, and now need to assign the name of the columns manually:&lt;/p&gt;
&lt;pre class=&#34;r numberLines&#34;&gt;&lt;code&gt;ds1 &amp;lt;- ds0 # duplicate to preserve the original
# because we removed the row with columns names during import:
names(ds1) &amp;lt;- c(&amp;quot;race&amp;quot;,&amp;quot;ethnicity&amp;quot;,&amp;quot;sex&amp;quot;,&amp;quot;age_group&amp;quot;,&amp;quot;age&amp;quot;,as.character(c(2006:2020)),&amp;quot;total&amp;quot;)
ds1 %&amp;gt;% dplyr::glimpse(90)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Observations: 807
Variables: 21
$ race      &amp;lt;chr&amp;gt; &amp;quot;White&amp;quot;, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...
$ ethnicity &amp;lt;chr&amp;gt; &amp;quot;Hispanic&amp;quot;, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,...
$ sex       &amp;lt;chr&amp;gt; &amp;quot;Female&amp;quot;, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N...
$ age_group &amp;lt;chr&amp;gt; &amp;quot;&amp;lt;1&amp;quot;, NA, &amp;quot;43834&amp;quot;, NA, NA, NA, NA, &amp;quot;43960&amp;quot;, NA, NA, NA, NA, NA, &amp;quot;44...
$ age       &amp;lt;chr&amp;gt; &amp;quot;0&amp;quot;, &amp;quot;Total&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;4&amp;quot;, &amp;quot;Total&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;7&amp;quot;, &amp;quot;8&amp;quot;, &amp;quot;9&amp;quot;,...
$ `2006`    &amp;lt;dbl&amp;gt; 33295, 33295, 24323, 24323, 24323, 24323, 97292, 25382, 25382, 2538...
$ `2007`    &amp;lt;dbl&amp;gt; 32276, 32276, 25841, 25841, 25841, 25841, 103364, 26712, 26712, 267...
$ `2008`    &amp;lt;dbl&amp;gt; 30429, 30429, 28693, 28693, 28693, 28693, 114772, 27637, 27637, 276...
$ `2009`    &amp;lt;dbl&amp;gt; 29468, 29468, 29927, 29919, 29917, 29853, 119616, 28378, 28378, 283...
$ `2010`    &amp;lt;dbl&amp;gt; 33976, 33976, 24145, 24145, 24145, 24145, 96580, 25981, 25943, 2594...
$ `2011`    &amp;lt;dbl&amp;gt; 27302, 27302, 27206, 27206, 27206, 27206, 108825, 26380, 26380, 263...
$ `2012`    &amp;lt;dbl&amp;gt; 26795, 26795, 27566, 27566, 27566, 27566, 110264, 26817, 26817, 268...
$ `2013`    &amp;lt;dbl&amp;gt; 27240, 27240, 27774, 27774, 27774, 27774, 111097, 27613, 27613, 276...
$ `2014`    &amp;lt;dbl&amp;gt; 28288, 28288, 27816, 27816, 27816, 27816, 111262, 28375, 28375, 283...
$ `2015`    &amp;lt;dbl&amp;gt; 29203, 29203, 28638, 28638, 28638, 28638, 114550, 28991, 28991, 289...
$ `2016`    &amp;lt;dbl&amp;gt; 29889, 29889, 29643, 29643, 29643, 29643, 118572, 29658, 29658, 296...
$ `2017`    &amp;lt;dbl&amp;gt; 30593, 30593, 30386, 30386, 30386, 30386, 121545, 30362, 30362, 303...
$ `2018`    &amp;lt;dbl&amp;gt; 30399, 30399, 31398, 31398, 31398, 31398, 125592, 31198, 31198, 311...
$ `2019`    &amp;lt;dbl&amp;gt; 31781, 31781, 31781, 31781, 31781, 31781, 127125, 31937, 31937, 319...
$ `2020`    &amp;lt;dbl&amp;gt; 32313, 32313, 32313, 32313, 32313, 32313, 129252, 32476, 32476, 324...
$ total     &amp;lt;dbl&amp;gt; 453247, 453247, 427450, 427442, 427440, 427376, 1709708, 427898, 42...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r numberLines&#34;&gt;&lt;code&gt;# Note, we will create a separate name (e.g. `ds1`, `ds2`, `ds3`, etc) only for the dataframes 
#we intend to keep, otherwise we will overwrite the existing to augment with trivial changes&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tweak-2-and-3&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tweak 2 and 3&lt;/h3&gt;
&lt;p&gt;To address (2) we apply &lt;code&gt;tidyr::fill()&lt;/code&gt; which carries the observation forward until it encounters a non-empty cell. &lt;code&gt;dplyr::filter&lt;/code&gt; addresses (3), removing observations that store total counts, while &lt;code&gt;stringr::str_replace&lt;/code&gt; recodes values that got misinterpreted as dates.&lt;/p&gt;
&lt;pre class=&#34;r numberLines&#34;&gt;&lt;code&gt;ds1 &amp;lt;- ds1 %&amp;gt;%
  # carry observations forward to fill cells missing due to Excel structure ()
  tidyr::fill(race, ethnicity,sex,age_group, age) %&amp;gt;% 
  dplyr::filter(!age == &amp;quot;Total&amp;quot;)  # because makes data untidy, we&amp;#39;ll compute later
ds1 %&amp;gt;% dplyr::glimpse(90)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Observations: 688
Variables: 21
$ race      &amp;lt;chr&amp;gt; &amp;quot;White&amp;quot;, &amp;quot;White&amp;quot;, &amp;quot;White&amp;quot;, &amp;quot;White&amp;quot;, &amp;quot;White&amp;quot;, &amp;quot;White&amp;quot;, &amp;quot;White&amp;quot;, &amp;quot;Whi...
$ ethnicity &amp;lt;chr&amp;gt; &amp;quot;Hispanic&amp;quot;, &amp;quot;Hispanic&amp;quot;, &amp;quot;Hispanic&amp;quot;, &amp;quot;Hispanic&amp;quot;, &amp;quot;Hispanic&amp;quot;, &amp;quot;Hispan...
$ sex       &amp;lt;chr&amp;gt; &amp;quot;Female&amp;quot;, &amp;quot;Female&amp;quot;, &amp;quot;Female&amp;quot;, &amp;quot;Female&amp;quot;, &amp;quot;Female&amp;quot;, &amp;quot;Female&amp;quot;, &amp;quot;Female...
$ age_group &amp;lt;chr&amp;gt; &amp;quot;&amp;lt;1&amp;quot;, &amp;quot;43834&amp;quot;, &amp;quot;43834&amp;quot;, &amp;quot;43834&amp;quot;, &amp;quot;43834&amp;quot;, &amp;quot;43960&amp;quot;, &amp;quot;43960&amp;quot;, &amp;quot;43960&amp;quot;...
$ age       &amp;lt;chr&amp;gt; &amp;quot;0&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;4&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;7&amp;quot;, &amp;quot;8&amp;quot;, &amp;quot;9&amp;quot;, &amp;quot;10&amp;quot;, &amp;quot;11&amp;quot;, &amp;quot;12&amp;quot;,...
$ `2006`    &amp;lt;dbl&amp;gt; 33295, 24323, 24323, 24323, 24323, 25382, 25382, 25382, 25382, 2538...
$ `2007`    &amp;lt;dbl&amp;gt; 32276, 25841, 25841, 25841, 25841, 26712, 26712, 26712, 26712, 2671...
$ `2008`    &amp;lt;dbl&amp;gt; 30429, 28693, 28693, 28693, 28693, 27637, 27637, 27637, 27637, 2763...
$ `2009`    &amp;lt;dbl&amp;gt; 29468, 29927, 29919, 29917, 29853, 28378, 28378, 28378, 28378, 2837...
$ `2010`    &amp;lt;dbl&amp;gt; 33976, 24145, 24145, 24145, 24145, 25981, 25943, 25943, 25914, 2591...
$ `2011`    &amp;lt;dbl&amp;gt; 27302, 27206, 27206, 27206, 27206, 26380, 26380, 26380, 26380, 2638...
$ `2012`    &amp;lt;dbl&amp;gt; 26795, 27566, 27566, 27566, 27566, 26817, 26817, 26817, 26817, 2681...
$ `2013`    &amp;lt;dbl&amp;gt; 27240, 27774, 27774, 27774, 27774, 27613, 27613, 27613, 27613, 2761...
$ `2014`    &amp;lt;dbl&amp;gt; 28288, 27816, 27816, 27816, 27816, 28375, 28375, 28375, 28375, 2837...
$ `2015`    &amp;lt;dbl&amp;gt; 29203, 28638, 28638, 28638, 28638, 28991, 28991, 28991, 28991, 2899...
$ `2016`    &amp;lt;dbl&amp;gt; 29889, 29643, 29643, 29643, 29643, 29658, 29658, 29658, 29658, 2965...
$ `2017`    &amp;lt;dbl&amp;gt; 30593, 30386, 30386, 30386, 30386, 30362, 30362, 30362, 30362, 3036...
$ `2018`    &amp;lt;dbl&amp;gt; 30399, 31398, 31398, 31398, 31398, 31198, 31198, 31198, 31198, 3119...
$ `2019`    &amp;lt;dbl&amp;gt; 31781, 31781, 31781, 31781, 31781, 31937, 31937, 31937, 31937, 3193...
$ `2020`    &amp;lt;dbl&amp;gt; 32313, 32313, 32313, 32313, 32313, 32476, 32476, 32476, 32476, 3247...
$ total     &amp;lt;dbl&amp;gt; 453247, 427450, 427442, 427440, 427376, 427898, 427860, 427860, 427...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tweak-4&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tweak 4&lt;/h3&gt;
&lt;p&gt;We need to do some investigation to address (4) and recode values that got misinterpreted as dates during the import&lt;/p&gt;
&lt;pre class=&#34;r numberLines&#34;&gt;&lt;code&gt;ds1 %&amp;gt;% dplyr::distinct(age_group) # to find out what strings got misinterpreted&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 13 x 1
   age_group
   &amp;lt;chr&amp;gt;    
 1 &amp;lt;1       
 2 43834    
 3 43960    
 4 44118    
 5 15-19    
 6 20-24    
 7 25-34    
 8 35-44    
 9 45-54    
10 55-64    
11 65-74    
12 75-84    
13 85+      &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r numberLines&#34;&gt;&lt;code&gt;ds1 %&amp;gt;% 
  dplyr::filter(age_group %in% c(&amp;quot;43834&amp;quot;,&amp;quot;43960&amp;quot;,&amp;quot;44118&amp;quot;)) %&amp;gt;% 
  dplyr::distinct(age_group, age) # to identify how to recode&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 14 x 2
   age_group age  
   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;
 1 43834     1    
 2 43834     2    
 3 43834     3    
 4 43834     4    
 5 43960     5    
 6 43960     6    
 7 43960     7    
 8 43960     8    
 9 43960     9    
10 44118     10   
11 44118     11   
12 44118     12   
13 44118     13   
14 44118     14   &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r numberLines&#34;&gt;&lt;code&gt;# imlement the recoding
ds1 &amp;lt;- ds1 %&amp;gt;%   
  dplyr::mutate(
    age_group = stringr::str_replace(age_group, &amp;quot;43834&amp;quot;, &amp;quot;1-4&amp;quot;)
    ,age_group = stringr::str_replace(age_group, &amp;quot;43960&amp;quot;, &amp;quot;5-9&amp;quot;)
    ,age_group = stringr::str_replace(age_group, &amp;quot;44118&amp;quot;, &amp;quot;10-14&amp;quot;)
  ) %&amp;gt;%
  dplyr::select(-total) # because it makes no sense in this context (adds up across the rows)
ds1 %&amp;gt;% glimpse(90)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Observations: 688
Variables: 20
$ race      &amp;lt;chr&amp;gt; &amp;quot;White&amp;quot;, &amp;quot;White&amp;quot;, &amp;quot;White&amp;quot;, &amp;quot;White&amp;quot;, &amp;quot;White&amp;quot;, &amp;quot;White&amp;quot;, &amp;quot;White&amp;quot;, &amp;quot;Whi...
$ ethnicity &amp;lt;chr&amp;gt; &amp;quot;Hispanic&amp;quot;, &amp;quot;Hispanic&amp;quot;, &amp;quot;Hispanic&amp;quot;, &amp;quot;Hispanic&amp;quot;, &amp;quot;Hispanic&amp;quot;, &amp;quot;Hispan...
$ sex       &amp;lt;chr&amp;gt; &amp;quot;Female&amp;quot;, &amp;quot;Female&amp;quot;, &amp;quot;Female&amp;quot;, &amp;quot;Female&amp;quot;, &amp;quot;Female&amp;quot;, &amp;quot;Female&amp;quot;, &amp;quot;Female...
$ age_group &amp;lt;chr&amp;gt; &amp;quot;&amp;lt;1&amp;quot;, &amp;quot;1-4&amp;quot;, &amp;quot;1-4&amp;quot;, &amp;quot;1-4&amp;quot;, &amp;quot;1-4&amp;quot;, &amp;quot;5-9&amp;quot;, &amp;quot;5-9&amp;quot;, &amp;quot;5-9&amp;quot;, &amp;quot;5-9&amp;quot;, &amp;quot;5-9&amp;quot;...
$ age       &amp;lt;chr&amp;gt; &amp;quot;0&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;4&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;7&amp;quot;, &amp;quot;8&amp;quot;, &amp;quot;9&amp;quot;, &amp;quot;10&amp;quot;, &amp;quot;11&amp;quot;, &amp;quot;12&amp;quot;,...
$ `2006`    &amp;lt;dbl&amp;gt; 33295, 24323, 24323, 24323, 24323, 25382, 25382, 25382, 25382, 2538...
$ `2007`    &amp;lt;dbl&amp;gt; 32276, 25841, 25841, 25841, 25841, 26712, 26712, 26712, 26712, 2671...
$ `2008`    &amp;lt;dbl&amp;gt; 30429, 28693, 28693, 28693, 28693, 27637, 27637, 27637, 27637, 2763...
$ `2009`    &amp;lt;dbl&amp;gt; 29468, 29927, 29919, 29917, 29853, 28378, 28378, 28378, 28378, 2837...
$ `2010`    &amp;lt;dbl&amp;gt; 33976, 24145, 24145, 24145, 24145, 25981, 25943, 25943, 25914, 2591...
$ `2011`    &amp;lt;dbl&amp;gt; 27302, 27206, 27206, 27206, 27206, 26380, 26380, 26380, 26380, 2638...
$ `2012`    &amp;lt;dbl&amp;gt; 26795, 27566, 27566, 27566, 27566, 26817, 26817, 26817, 26817, 2681...
$ `2013`    &amp;lt;dbl&amp;gt; 27240, 27774, 27774, 27774, 27774, 27613, 27613, 27613, 27613, 2761...
$ `2014`    &amp;lt;dbl&amp;gt; 28288, 27816, 27816, 27816, 27816, 28375, 28375, 28375, 28375, 2837...
$ `2015`    &amp;lt;dbl&amp;gt; 29203, 28638, 28638, 28638, 28638, 28991, 28991, 28991, 28991, 2899...
$ `2016`    &amp;lt;dbl&amp;gt; 29889, 29643, 29643, 29643, 29643, 29658, 29658, 29658, 29658, 2965...
$ `2017`    &amp;lt;dbl&amp;gt; 30593, 30386, 30386, 30386, 30386, 30362, 30362, 30362, 30362, 3036...
$ `2018`    &amp;lt;dbl&amp;gt; 30399, 31398, 31398, 31398, 31398, 31198, 31198, 31198, 31198, 3119...
$ `2019`    &amp;lt;dbl&amp;gt; 31781, 31781, 31781, 31781, 31781, 31937, 31937, 31937, 31937, 3193...
$ `2020`    &amp;lt;dbl&amp;gt; 32313, 32313, 32313, 32313, 32313, 32476, 32476, 32476, 32476, 3247...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tweak-5&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tweak 5&lt;/h3&gt;
&lt;p&gt;Now we need to recode the values of &lt;code&gt;age&lt;/code&gt; into a new grouping variable &lt;code&gt;age_group5&lt;/code&gt;, which will correct the unevennes of the original grouping in &lt;code&gt;age_group&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r numberLines&#34;&gt;&lt;code&gt;ds1$age_group5 &amp;lt;- ds1$age_group # because we want to keep the original for quality check
ds1$age_group5[ds1$age %in% c(0:4)]   &amp;lt;- &amp;quot;0-4&amp;quot;
ds1$age_group5[ds1$age %in% c(25:29)] &amp;lt;- &amp;quot;25-29&amp;quot;
ds1$age_group5[ds1$age %in% c(30:34)] &amp;lt;- &amp;quot;30-34&amp;quot;
ds1$age_group5[ds1$age %in% c(35:39)] &amp;lt;- &amp;quot;35-39&amp;quot;
ds1$age_group5[ds1$age %in% c(40:44)] &amp;lt;- &amp;quot;40-44&amp;quot;
ds1$age_group5[ds1$age %in% c(45:49)] &amp;lt;- &amp;quot;45-49&amp;quot;
ds1$age_group5[ds1$age %in% c(50:54)] &amp;lt;- &amp;quot;50-54&amp;quot;
ds1$age_group5[ds1$age %in% c(55:59)] &amp;lt;- &amp;quot;55-59&amp;quot;
ds1$age_group5[ds1$age %in% c(60:64)] &amp;lt;- &amp;quot;60-64&amp;quot;
ds1$age_group5[ds1$age %in% c(65:69)] &amp;lt;- &amp;quot;65-69&amp;quot;
ds1$age_group5[ds1$age %in% c(70:74)] &amp;lt;- &amp;quot;70-74&amp;quot;
ds1$age_group5[ds1$age %in% c(75:79)] &amp;lt;- &amp;quot;75-79&amp;quot;
ds1$age_group5[ds1$age %in% c(80:84)] &amp;lt;- &amp;quot;80-84&amp;quot;

ds1 %&amp;gt;% dplyr::distinct(age_group, age_group5) # to inspect the result&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 19 x 2
   age_group age_group5
   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;     
 1 &amp;lt;1        0-4       
 2 1-4       0-4       
 3 5-9       5-9       
 4 10-14     10-14     
 5 15-19     15-19     
 6 20-24     20-24     
 7 25-34     25-29     
 8 25-34     30-34     
 9 35-44     35-39     
10 35-44     40-44     
11 45-54     45-49     
12 45-54     50-54     
13 55-64     55-59     
14 55-64     60-64     
15 65-74     65-69     
16 65-74     70-74     
17 75-84     75-79     
18 75-84     80-84     
19 85+       85+       &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;pivot-long&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Pivot Long&lt;/h3&gt;
&lt;p&gt;To enable a more seemless graph making, as well as to provide a more convenient shape for subsequent aggregation over age groups, we need to &lt;code&gt;tidyr:pivot_long&lt;/code&gt; (formely known as &lt;code&gt;tidyr::gather&lt;/code&gt;) the columns storing population estimates for each year.&lt;/p&gt;
&lt;pre class=&#34;r numberLines&#34;&gt;&lt;code&gt;# to translate into a longer form with respect to year
ds2 &amp;lt;- ds1 %&amp;gt;%
  tidyr::pivot_longer(cols = as.character(2006:2020),names_to = &amp;quot;year&amp;quot;, values_to = &amp;quot;count&amp;quot;) 
ds2 %&amp;gt;% dplyr::glimpse(90)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Observations: 10,320
Variables: 8
$ race       &amp;lt;chr&amp;gt; &amp;quot;White&amp;quot;, &amp;quot;White&amp;quot;, &amp;quot;White&amp;quot;, &amp;quot;White&amp;quot;, &amp;quot;White&amp;quot;, &amp;quot;White&amp;quot;, &amp;quot;White&amp;quot;, &amp;quot;Wh...
$ ethnicity  &amp;lt;chr&amp;gt; &amp;quot;Hispanic&amp;quot;, &amp;quot;Hispanic&amp;quot;, &amp;quot;Hispanic&amp;quot;, &amp;quot;Hispanic&amp;quot;, &amp;quot;Hispanic&amp;quot;, &amp;quot;Hispa...
$ sex        &amp;lt;chr&amp;gt; &amp;quot;Female&amp;quot;, &amp;quot;Female&amp;quot;, &amp;quot;Female&amp;quot;, &amp;quot;Female&amp;quot;, &amp;quot;Female&amp;quot;, &amp;quot;Female&amp;quot;, &amp;quot;Femal...
$ age_group  &amp;lt;chr&amp;gt; &amp;quot;&amp;lt;1&amp;quot;, &amp;quot;&amp;lt;1&amp;quot;, &amp;quot;&amp;lt;1&amp;quot;, &amp;quot;&amp;lt;1&amp;quot;, &amp;quot;&amp;lt;1&amp;quot;, &amp;quot;&amp;lt;1&amp;quot;, &amp;quot;&amp;lt;1&amp;quot;, &amp;quot;&amp;lt;1&amp;quot;, &amp;quot;&amp;lt;1&amp;quot;, &amp;quot;&amp;lt;1&amp;quot;, &amp;quot;&amp;lt;1&amp;quot;, ...
$ age        &amp;lt;chr&amp;gt; &amp;quot;0&amp;quot;, &amp;quot;0&amp;quot;, &amp;quot;0&amp;quot;, &amp;quot;0&amp;quot;, &amp;quot;0&amp;quot;, &amp;quot;0&amp;quot;, &amp;quot;0&amp;quot;, &amp;quot;0&amp;quot;, &amp;quot;0&amp;quot;, &amp;quot;0&amp;quot;, &amp;quot;0&amp;quot;, &amp;quot;0&amp;quot;, &amp;quot;0&amp;quot;, &amp;quot;...
$ age_group5 &amp;lt;chr&amp;gt; &amp;quot;0-4&amp;quot;, &amp;quot;0-4&amp;quot;, &amp;quot;0-4&amp;quot;, &amp;quot;0-4&amp;quot;, &amp;quot;0-4&amp;quot;, &amp;quot;0-4&amp;quot;, &amp;quot;0-4&amp;quot;, &amp;quot;0-4&amp;quot;, &amp;quot;0-4&amp;quot;, &amp;quot;0-...
$ year       &amp;lt;chr&amp;gt; &amp;quot;2006&amp;quot;, &amp;quot;2007&amp;quot;, &amp;quot;2008&amp;quot;, &amp;quot;2009&amp;quot;, &amp;quot;2010&amp;quot;, &amp;quot;2011&amp;quot;, &amp;quot;2012&amp;quot;, &amp;quot;2013&amp;quot;, &amp;quot;2...
$ count      &amp;lt;dbl&amp;gt; 33295, 32276, 30429, 29468, 33976, 27302, 26795, 27240, 28288, 292...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r numberLines&#34;&gt;&lt;code&gt;# `ds2` is the source set for computing totals because it has both `age_group`` categories&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;factors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Factors&lt;/h3&gt;
&lt;p&gt;Before aggregating over &lt;code&gt;age_group&lt;/code&gt; and &lt;code&gt;age_group5&lt;/code&gt; let us transform string variables into factors. so that we don’t have to do it in the individual data sets.&lt;/p&gt;
&lt;pre class=&#34;r numberLines&#34;&gt;&lt;code&gt;lvl_age_groups &amp;lt;-c(
  &amp;quot;&amp;lt;1&amp;quot;
  ,&amp;quot;1-4&amp;quot;
  ,&amp;quot;5-9&amp;quot;
  ,&amp;quot;10-14&amp;quot;
  ,&amp;quot;15-19&amp;quot;
  ,&amp;quot;20-24&amp;quot;
  ,&amp;quot;25-34&amp;quot;
  ,&amp;quot;35-44&amp;quot;
  ,&amp;quot;45-54&amp;quot;
  ,&amp;quot;55-64&amp;quot;
  ,&amp;quot;65-74&amp;quot;
  ,&amp;quot;75-84&amp;quot;
  ,&amp;quot;85+&amp;quot;
)

lvl_age_groups5 &amp;lt;- c(
  &amp;quot;0-4&amp;quot;
  ,&amp;quot;5-9&amp;quot;
  ,&amp;quot;10-14&amp;quot;
  ,&amp;quot;15-19&amp;quot;
  ,&amp;quot;20-24&amp;quot;
  ,&amp;quot;25-29&amp;quot;
  ,&amp;quot;30-34&amp;quot;
  ,&amp;quot;35-39&amp;quot;
  ,&amp;quot;40-44&amp;quot;
  ,&amp;quot;45-49&amp;quot;
  ,&amp;quot;50-54&amp;quot;
  ,&amp;quot;55-59&amp;quot;
  ,&amp;quot;60-64&amp;quot;
  ,&amp;quot;65-69&amp;quot;
  ,&amp;quot;70-74&amp;quot;
  ,&amp;quot;75-79&amp;quot;
  ,&amp;quot;80-84&amp;quot;
  ,&amp;quot;85+&amp;quot;
)
ds2 &amp;lt;- ds2 %&amp;gt;%
  dplyr::mutate(
    race_ethnicity = factor(paste0(race, &amp;quot; + &amp;quot;, ethnicity))
    ,race          = factor(race)
    ,ethnicity     = factor(ethnicity)
    ,age_group     = factor(age_group, levels = lvl_age_groups)
    ,age_group5    = factor(age_group5, levels = lvl_age_groups5)
    ,age           = as.integer(age)
    ,sex           = factor(sex)
    ,year          = as.integer(year)
  ) %&amp;gt;% 
  dplyr::select(race, ethnicity, age_group, age_group5, dplyr::everything())
ds2 %&amp;gt;% dplyr::glimpse(90)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Observations: 10,320
Variables: 9
$ race           &amp;lt;fct&amp;gt; White, White, White, White, White, White, White, White, White,...
$ ethnicity      &amp;lt;fct&amp;gt; Hispanic, Hispanic, Hispanic, Hispanic, Hispanic, Hispanic, Hi...
$ age_group      &amp;lt;fct&amp;gt; &amp;lt;1, &amp;lt;1, &amp;lt;1, &amp;lt;1, &amp;lt;1, &amp;lt;1, &amp;lt;1, &amp;lt;1, &amp;lt;1, &amp;lt;1, &amp;lt;1, &amp;lt;1, &amp;lt;1, &amp;lt;1, &amp;lt;1, 1-...
$ age_group5     &amp;lt;fct&amp;gt; 0-4, 0-4, 0-4, 0-4, 0-4, 0-4, 0-4, 0-4, 0-4, 0-4, 0-4, 0-4, 0-...
$ sex            &amp;lt;fct&amp;gt; Female, Female, Female, Female, Female, Female, Female, Female...
$ age            &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,...
$ year           &amp;lt;int&amp;gt; 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 20...
$ count          &amp;lt;dbl&amp;gt; 33295, 32276, 30429, 29468, 33976, 27302, 26795, 27240, 28288,...
$ race_ethnicity &amp;lt;fct&amp;gt; White + Hispanic, White + Hispanic, White + Hispanic, White + ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;aggregate&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Aggregate&lt;/h3&gt;
&lt;p&gt;Note that at this point, each row contains a population estimate for a given age in years. However, as you might have noticed that counts for each year of age are not unique:&lt;/p&gt;
&lt;pre class=&#34;r numberLines&#34;&gt;&lt;code&gt;ds2 %&amp;gt;% 
  dplyr::filter(
    age %in% c(25:39), race_ethnicity == &amp;quot;White + Hispanic&amp;quot;,sex == &amp;quot;Female&amp;quot;, year==2018
  ) %&amp;gt;% 
  dplyr::select(race, ethnicity, age_group, age_group5, sex, age,count)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 15 x 7
   race  ethnicity age_group age_group5 sex      age count
   &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;     &amp;lt;fct&amp;gt;     &amp;lt;fct&amp;gt;      &amp;lt;fct&amp;gt;  &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
 1 White Hispanic  25-34     25-29      Female    25 34479
 2 White Hispanic  25-34     25-29      Female    26 34479
 3 White Hispanic  25-34     25-29      Female    27 34479
 4 White Hispanic  25-34     25-29      Female    28 34479
 5 White Hispanic  25-34     25-29      Female    29 34479
 6 White Hispanic  25-34     30-34      Female    30 35056
 7 White Hispanic  25-34     30-34      Female    31 35056
 8 White Hispanic  25-34     30-34      Female    32 35056
 9 White Hispanic  25-34     30-34      Female    33 35056
10 White Hispanic  25-34     30-34      Female    34 35056
11 White Hispanic  35-44     35-39      Female    35 36549
12 White Hispanic  35-44     35-39      Female    36 36549
13 White Hispanic  35-44     35-39      Female    37 36549
14 White Hispanic  35-44     35-39      Female    38 36549
15 White Hispanic  35-44     35-39      Female    39 36549&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In fact, it appears that Florida Health Charts computes the total for 5-year category and then devides it evenly among constituent elements of the &lt;code&gt;age_category&lt;/code&gt;. My guess, this deals with privacy guidelines. Therefore, the most granualar age break up is only 5-year categories.&lt;/p&gt;
&lt;p&gt;To preserve the original grouping we create two separate datasets, each providing the totals for respective age category.&lt;/p&gt;
&lt;pre class=&#34;r numberLines&#34;&gt;&lt;code&gt;ds_age_group &amp;lt;- ds2 %&amp;gt;% 
  dplyr::group_by(race, ethnicity, sex, age_group, year) %&amp;gt;% 
  dplyr::summarize(
    count = sum(count, na.rm = T)
  ) %&amp;gt;% 
  dplyr::ungroup() %&amp;gt;% 
  dplyr::mutate(
    race_ethnicity = paste0(race, &amp;quot; + &amp;quot;, ethnicity),
    race_ethnicity = factor(race_ethnicity)
  )

ds_age_group5 &amp;lt;- ds2 %&amp;gt;% 
  dplyr::group_by(race, ethnicity, sex, age_group5, year) %&amp;gt;% 
  dplyr::summarize(
    count = sum(count, na.rm = T)
  ) %&amp;gt;% 
  dplyr::ungroup() %&amp;gt;% 
  dplyr::mutate(
    race_ethnicity = paste0(race, &amp;quot; + &amp;quot;, ethnicity),
    race_ethnicity = factor(race_ethnicity)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is the &lt;code&gt;ds_age_group5&lt;/code&gt; that will be focus of subsequent graphs. However, to better demonstrate why we needed to create new grouping of ages, we will preserve both dataframes.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;save-to-disk&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Save to disk&lt;/h2&gt;
&lt;pre class=&#34;r numberLines&#34;&gt;&lt;code&gt;list(
  &amp;quot;ds_wide&amp;quot;        = ds1
  ,&amp;quot;ds_long&amp;quot;       = ds2
  ,&amp;quot;ds_age_group&amp;quot;  = ds_age_group
  ,&amp;quot;ds_age_group3&amp;quot; = ds_age_group5
) %&amp;gt;% 
  saveRDS(&amp;quot;./content/post/2020-03-27-florida-demographic-growth/data/clean_data.rds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The cleaned version of this dataset is available for download &lt;a href=&#34;https://github.com/dss-hmi/suicide-prevention-2019/raw/3ff78365931214e342640523f2096c2eac443b2f/analysis/blogposts/florida-demographic-growth/data/clean_data.rds&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;graphing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Graphing&lt;/h1&gt;
&lt;div id=&#34;total-population&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Total population&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;How does the total population of Florida changes between 2006 and 2020?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r numberLines&#34;&gt;&lt;code&gt;# Total population of Florida over the years
ds_age_group5 %&amp;gt;% 
  dplyr::group_by(year) %&amp;gt;% 
  dplyr::summarize(
    count = sum(count, na.rm = T)
  ) %&amp;gt;% 
  ggplot(aes(x=year, y = count))+
  geom_point()+
  geom_line()+
  scale_y_continuous(labels = scales::comma)+
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;figure_rmd/g0-1.png&#34; width=&#34;900px&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ethnic-groups-together&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ethnic groups: Together&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;How does each ethnic group change during this time?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r numberLines&#34;&gt;&lt;code&gt;# total population of Florida by broken down by 4 ethnic groups (race_ethnicity)
d1 &amp;lt;- ds_age_group5 %&amp;gt;% 
  dplyr::group_by(race_ethnicity, year) %&amp;gt;% 
  dplyr::summarize(
    n_people = sum(count, rm.na = T)
  )
g1 &amp;lt;- d1 %&amp;gt;% 
  ggplot(aes(x = year,  y = n_people, color = race_ethnicity))+
  geom_line(aes(group = race_ethnicity))+
  geom_point(shape = 21, fill = NA, size =2)+
  scale_y_continuous(labels = scales::comma)+
  theme_bw()+
  theme(
    axis.text.x = element_text(angle = - 90,vjust =.5, hjust = -0)
    #https://stackoverflow.com/questions/1330989/rotating-and-spacing-axis-labels-in-ggplot2
  )+
  labs(
    title = &amp;quot;Population growth in Florida over last 15 years \n  broken down by ethnic groups&amp;quot;
    ,color = &amp;quot;Ethnic Group&amp;quot;
    ,x = &amp;quot;Calendar Year&amp;quot;
    ,y = &amp;quot;Population Count&amp;quot;
  )
g1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;figure_rmd/g1-1.png&#34; width=&#34;900px&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ethnic-groups-separate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ethnic groups: Separate&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;what Ethnic group is most dissimilar from the other three in their dynamics?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r numberLines&#34;&gt;&lt;code&gt;# Q: what Ethnic group is most dissimilar from the other three in their dynamics?
g1 + facet_wrap(~race_ethnicity, scale = &amp;quot;free_y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;figure_rmd/q1-1.png&#34; width=&#34;900px&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r numberLines&#34;&gt;&lt;code&gt;# A: &amp;quot;White + Non-Hispanic&amp;quot; because of a &amp;quot;dip&amp;quot; in late 2000&amp;#39;s&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;age-composition-in-2019&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Age composition in 2019&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;What is the age composition of each ethnic group in 2019?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r numberLines&#34;&gt;&lt;code&gt;# Build a graph showing age composition of all ethnic groups in 2019
g2 &amp;lt;- ds_age_group5 %&amp;gt;%
  dplyr::filter(year == 2019) %&amp;gt;%
  ggplot(aes(x = age_group5, y = count, fill = race_ethnicity)) +
  geom_col()+
  facet_grid(sex ~ race_ethnicity)+
  scale_y_continuous(labels = scales::comma)+
  # https://stackoverflow.com/questions/14563989/force-r-to-stop-plotting-abbreviated-axis-labels-e-g-1e00-in-ggplot2 also https://r-graphics.org/recipe-axes-tick-label
  theme_bw()+
  theme(
     axis.text.x = element_text(angle = - 90,vjust =.5, hjust = -0)
     #https://stackoverflow.com/questions/1330989/rotating-and-spacing-axis-labels-in-ggplot2
  )+
  labs(
    title = &amp;quot;Population in Florida in 2019 broken down by age groups and gender&amp;quot;
    ,color = &amp;quot;Ethnic Group&amp;quot;
    ,x = &amp;quot;Calendar Year&amp;quot;
    ,y = &amp;quot;Population Count&amp;quot;
  )
g2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;figure_rmd/g2-1.png&#34; width=&#34;900px&#34; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What would it look like if we used the original &lt;code&gt;age_group&lt;/code&gt;?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r numberLines&#34;&gt;&lt;code&gt;g2a &amp;lt;- ds_age_group %&amp;gt;%
  dplyr::filter(year == 2019) %&amp;gt;%
  ggplot(aes(x = age_group, y = count, fill = race_ethnicity)) +
  geom_col()+
  facet_grid(sex ~ race_ethnicity)+
  scale_y_continuous(labels = scales::comma)+
  # https://stackoverflow.com/questions/14563989/force-r-to-stop-plotting-abbreviated-axis-labels-e-g-1e00-in-ggplot2 also https://r-graphics.org/recipe-axes-tick-label
  theme_bw()+
  theme(
    axis.text.x = element_text(angle = - 90,vjust =.5, hjust = -0)
    #https://stackoverflow.com/questions/1330989/rotating-and-spacing-axis-labels-in-ggplot2
  )+
  labs(
    title = &amp;quot;Population in Florida in 2019 broken down by age groups and gender&amp;quot;
    ,color = &amp;quot;Ethnic Group&amp;quot;
    ,x = &amp;quot;Calendar Year&amp;quot;
    ,y = &amp;quot;Population Count&amp;quot;
  )
g2a&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;figure_rmd/q2-1.png&#34; width=&#34;900px&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;session-information&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;session information&lt;/h1&gt;
&lt;p&gt;For the sake of documentation and reproducibility, the current report was rendered in the following environment. Click the line below to expand.&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Environment &lt;span class=&#34;glyphicon glyphicon-plus-sign&#34;&gt;&lt;/span&gt;&lt;/summary&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- Session info -------------------------------------------------------------------------------------------------------
 setting  value                       
 version  R version 3.6.2 (2019-12-12)
 os       Windows 10 x64              
 system   x86_64, mingw32             
 ui       RTerm                       
 language (EN)                        
 collate  English_United States.1252  
 ctype    English_United States.1252  
 tz       America/New_York            
 date     2020-03-29                  

- Packages -----------------------------------------------------------------------------------------------------------
 package     * version date       lib source        
 assertthat    0.2.1   2019-03-21 [1] CRAN (R 3.6.2)
 backports     1.1.5   2019-10-02 [1] CRAN (R 3.6.1)
 blogdown      0.17    2019-11-13 [1] CRAN (R 3.6.2)
 bookdown      0.17    2020-01-11 [1] CRAN (R 3.6.2)
 callr         3.4.2   2020-02-12 [1] CRAN (R 3.6.2)
 cellranger    1.1.0   2016-07-27 [1] CRAN (R 3.6.2)
 cli           2.0.1   2020-01-08 [1] CRAN (R 3.6.2)
 colorspace    1.4-1   2019-03-18 [1] CRAN (R 3.6.1)
 crayon        1.3.4   2017-09-16 [1] CRAN (R 3.6.2)
 desc          1.2.0   2018-05-01 [1] CRAN (R 3.6.2)
 devtools      2.2.2   2020-02-17 [1] CRAN (R 3.6.3)
 digest        0.6.24  2020-02-12 [1] CRAN (R 3.6.2)
 dplyr       * 0.8.4   2020-01-31 [1] CRAN (R 3.6.2)
 ellipsis      0.3.0   2019-09-20 [1] CRAN (R 3.6.2)
 evaluate      0.14    2019-05-28 [1] CRAN (R 3.6.2)
 fansi         0.4.1   2020-01-08 [1] CRAN (R 3.6.2)
 fs            1.3.1   2019-05-06 [1] CRAN (R 3.6.2)
 ggplot2     * 3.2.1   2019-08-10 [1] CRAN (R 3.6.2)
 ggpubr        0.2.5   2020-02-13 [1] CRAN (R 3.6.2)
 ggsignif      0.6.0   2019-08-08 [1] CRAN (R 3.6.2)
 glue          1.3.1   2019-03-12 [1] CRAN (R 3.6.2)
 gtable        0.3.0   2019-03-25 [1] CRAN (R 3.6.2)
 htmltools     0.4.0   2019-10-04 [1] CRAN (R 3.6.2)
 knitr       * 1.28    2020-02-06 [1] CRAN (R 3.6.2)
 lazyeval      0.2.2   2019-03-15 [1] CRAN (R 3.6.2)
 lifecycle     0.1.0   2019-08-01 [1] CRAN (R 3.6.2)
 magrittr    * 1.5     2014-11-22 [1] CRAN (R 3.6.2)
 memoise       1.1.0   2017-04-21 [1] CRAN (R 3.6.2)
 munsell       0.5.0   2018-06-12 [1] CRAN (R 3.6.2)
 pillar        1.4.3   2019-12-20 [1] CRAN (R 3.6.2)
 pkgbuild      1.0.6   2019-10-09 [1] CRAN (R 3.6.2)
 pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 3.6.2)
 pkgload       1.0.2   2018-10-29 [1] CRAN (R 3.6.2)
 prettyunits   1.1.1   2020-01-24 [1] CRAN (R 3.6.2)
 processx      3.4.2   2020-02-09 [1] CRAN (R 3.6.2)
 ps            1.3.2   2020-02-13 [1] CRAN (R 3.6.2)
 purrr         0.3.3   2019-10-18 [1] CRAN (R 3.6.2)
 R6            2.4.1   2019-11-12 [1] CRAN (R 3.6.2)
 Rcpp          1.0.3   2019-11-08 [1] CRAN (R 3.6.2)
 readxl        1.3.1   2019-03-13 [1] CRAN (R 3.6.2)
 remotes       2.1.1   2020-02-15 [1] CRAN (R 3.6.2)
 rlang         0.4.4   2020-01-28 [1] CRAN (R 3.6.2)
 rmarkdown     2.1     2020-01-20 [1] CRAN (R 3.6.2)
 rprojroot     1.3-2   2018-01-03 [1] CRAN (R 3.6.2)
 scales        1.1.0   2019-11-18 [1] CRAN (R 3.6.2)
 sessioninfo   1.1.1   2018-11-05 [1] CRAN (R 3.6.2)
 stringi       1.4.5   2020-01-11 [1] CRAN (R 3.6.2)
 stringr       1.4.0   2019-02-10 [1] CRAN (R 3.6.2)
 testthat      2.3.1   2019-12-01 [1] CRAN (R 3.6.2)
 tibble        2.1.3   2019-06-06 [1] CRAN (R 3.6.2)
 tidyr         1.0.2   2020-01-24 [1] CRAN (R 3.6.2)
 tidyselect    1.0.0   2020-01-27 [1] CRAN (R 3.6.2)
 usethis       1.5.1   2019-07-04 [1] CRAN (R 3.6.2)
 utf8          1.1.4   2018-05-24 [1] CRAN (R 3.6.2)
 vctrs         0.2.2   2020-01-24 [1] CRAN (R 3.6.2)
 withr         2.1.2   2018-03-15 [1] CRAN (R 3.6.2)
 xfun          0.12    2020-01-13 [1] CRAN (R 3.6.2)
 yaml          2.2.1   2020-02-01 [1] CRAN (R 3.6.2)

[1] C:/Users/an499583/Documents/R/win-library/3.6
[2] C:/Program Files/R/R-3.6.2/library&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Managing Data Analysis with RStudio</title>
      <link>/post/2020-01-07-graphing-models-with-titanic-data/</link>
      <pubDate>Tue, 07 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/2020-01-07-graphing-models-with-titanic-data/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Recently I was invited to give a workshop on data analysis with R at the training retreat for Health System Impact Fellowship by Canadian Institutes of Health Research.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/CIHR_ImpactFellows?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#CIHR_ImpactFellows&lt;/a&gt; immersed in a hands-on RStudio workshop, led by &lt;a href=&#34;https://twitter.com/andkovpro?ref_src=twsrc%5Etfw&#34;&gt;@andkovpro&lt;/a&gt;, Assistant Prof @ UCF and alumnus of the 2017 fellow cohort. Thank you &lt;a href=&#34;https://twitter.com/andkovpro?ref_src=twsrc%5Etfw&#34;&gt;@andkovpro&lt;/a&gt; ! &lt;a href=&#34;https://twitter.com/hashtag/HSIF2019?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#HSIF2019&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/enrichedcorecomps?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#enrichedcorecomps&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/datascience?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#datascience&lt;/a&gt; &lt;a href=&#34;https://t.co/P0Qp3saMQF&#34;&gt;pic.twitter.com/P0Qp3saMQF&lt;/a&gt;&lt;/p&gt;&amp;mdash; Meghan McMahon (@McMahon_Meg) &lt;a href=&#34;https://twitter.com/McMahon_Meg/status/1199435489897517059?ref_src=twsrc%5Etfw&#34;&gt;November 26, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;The workshop was hosted at the picturesque Hart House of the University of Toronto:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;university-toronto-courtyard.jpg&#34; alt=&#34;courtyard&#34; /&gt;
The workshop involved&lt;/p&gt;
&lt;div id=&#34;audience&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Audience&lt;/h1&gt;
&lt;p&gt;My audience constisted of postdoctoral researchers and Ph.D. students, few of whom had experience with R, however majority have taken 3 or more courses in statistics and applied analysis. To gain better understanding of their background, I have asked them to fill out a brief survey (see results in my &lt;a href=&#34;https://raw.githack.com/andkov/hsif-2019-data-analysis/master/libs/materials/HSIF-Toronto-2019-11-26-data-analysis-workshop.pdf&#34;&gt;slides&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;I had about 90 minutes, so I reasoned that instead of overwhelming them with technical information, which would leave little trace in their fatigued minds ( workshop was at the end of the day), I should create a resource that they would be able to use in the future. The time in workshop, then, should spent on gentle introduction to data analysis with R using the examples from this resource. This also gave my workshop the flexibility to accomodate learners of various skill levels: more advanced participants will have the material to study on their own if the talk leaves them underchallenged in any given point in time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;learning-objectives&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Learning Objectives&lt;/h1&gt;
&lt;p&gt;I wanted the audience to be exposed to examples of performing the following tasks:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Organizing data analysis in a RMarkdown document&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Graphing the predictions of a statistical model (logistic regression)&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Jump-starting an analytic project using a project template&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;deliverables&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Deliverables&lt;/h1&gt;
&lt;div id=&#34;organizing-data-analysis-in-a-rmarkdown-document&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;1. Organizing data analysis in a RMarkdown document&lt;/h3&gt;
&lt;p&gt;I have created two reports, containing identidcal code that implemented basic exploration of Titanic data with logistic regression:
- &lt;a href=&#34;https://raw.githack.com/andkov/hsif-2019-data-analysis/master/analysis/titanic-notebook-only/titanic-notebook.html&#34;&gt;notebook-only&lt;/a&gt; - a notebook combining code and annotation in the same &lt;code&gt;.Rmd&lt;/code&gt; file
- &lt;a href=&#34;https://raw.githack.com/andkov/hsif-2019-data-analysis/master/analysis/titanic-separate-layers/titanic.html&#34;&gt;separate-layers&lt;/a&gt; - a report separating the analytic layer (&lt;code&gt;.R&lt;/code&gt;) from the annotation layer (&lt;code&gt;.Rmd&lt;/code&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://raw.githack.com/andkov/hsif-2019-data-analysis/master/libs/materials/HSIF-Toronto-2019-11-26-data-analysis-workshop.pdf&#34;&gt;presentation slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;graphing-a-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Graphing a model&lt;/h2&gt;
&lt;p&gt;Carefull not to overwhelm with details, I chose to focus on a basic logistic regression model predicting survival in the Titanic data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Model 0
survived ~ sex

# Model 1
survived ~ sex + age

# Model 2
survived ~ sex + age + passenger_class

# Model 3
survived ~ sex + age + passenger_class + port_embarked
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, instead of focusing on interpreting the estimated parameters, I opted to generate predicted values and then to graph them to examine the effect respective variables would have on the binary outcome. We converted the log-odds into probabilities of the outcome (y-axis) and then mapped predictors on other visual dimension. To illustrate, the prediction for Model 2 &lt;code&gt;survived ~ sex + age + passenger_class&lt;/code&gt; looked like this:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;model2.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;model_2&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;materials&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Materials&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://raw.githack.com/andkov/hsif-2019-data-analysis/master/analysis/titanic-notebook-only/titanic-notebook.html&#34;&gt;notebook-only&lt;/a&gt; - analytic report as a notebook (combines code and annotation in the same &lt;code&gt;.Rmd&lt;/code&gt; file)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://raw.githack.com/andkov/hsif-2019-data-analysis/master/analysis/titanic-separate-layers/titanic.html&#34;&gt;separate-layers&lt;/a&gt; - analytic report separating the analytic layer (&lt;code&gt;.R&lt;/code&gt;) from the annotation layer (&lt;code&gt;.Rmd&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://raw.githack.com/andkov/hsif-2019-data-analysis/master/libs/materials/HSIF-Toronto-2019-11-26-data-analysis-workshop.pdf&#34;&gt;presentation slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;abstract&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;The workshop will review best practices of reproducible research including folder architecture, data preparation, graph making, statistical modeling, and script documentation. The workshop is targeted at researchers who are expected to conduct their own analysis of data and prepare reports that deliver the findings to both technical and executive audiences within health systems. Using logistic regression as an example, the participants will learn to communicate statistical findings more effectively, and will evaluate the advantages of using computational notebooks in RStudio to disseminate the results.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Managing Data Analysis with RStudio</title>
      <link>/talk/2019-11-26-hsif-toronto-workshop/</link>
      <pubDate>Tue, 26 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/talk/2019-11-26-hsif-toronto-workshop/</guid>
      <description>


&lt;div id=&#34;materials&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Materials&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://raw.githack.com/andkov/hsif-2019-data-analysis/master/analysis/titanic-notebook-only/titanic-notebook.html&#34;&gt;notebook-only&lt;/a&gt; - analytic report as a notebook (combines code and annotation in the same &lt;code&gt;.Rmd&lt;/code&gt; file)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://raw.githack.com/andkov/hsif-2019-data-analysis/master/analysis/titanic-separate-layers/titanic.html&#34;&gt;separate-layers&lt;/a&gt; - analytic report separating the analytic layer (&lt;code&gt;.R&lt;/code&gt;) from the annotation layer (&lt;code&gt;.Rmd&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://raw.githack.com/andkov/hsif-2019-data-analysis/master/libs/materials/HSIF-Toronto-2019-11-26-data-analysis-workshop.pdf&#34;&gt;presentation slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;abstract&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;The workshop will review best practices of reproducible research including folder architecture, data preparation, graph making, statistical modeling, and script documentation. The workshop is targeted at researchers who are expected to conduct their own analysis of data and prepare reports that deliver the findings to both technical and executive audiences within health systems. Using logistic regression as an example, the participants will learn to communicate statistical findings more effectively, and will evaluate the advantages of using computational notebooks in RStudio to disseminate the results.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Implementing Reproducible Visualizations</title>
      <link>/talk/2019-11-08-visualizing-logistic-regression/</link>
      <pubDate>Wed, 06 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/talk/2019-11-08-visualizing-logistic-regression/</guid>
      <description>


&lt;p&gt;Visualising results of statistical modeling is a key component of data science workflow. Statistical graphs often is the best means to explain and promote research findings. However,in order to find that one graph that tells the story worth sharing, we sometimes have to try out and sift through many data visualizations. How should we approach such a task? What can we do to make it easier from both production and evaluation perspectives? This talk will demonstrate a reproducible graphing system designed for the IPDLN-2018 hackathon.&lt;/p&gt;
&lt;p&gt;The system evaluates synthetic socioeconomic and mortality data with logistic regression. The data was prepared for the hackathon by Statistic Canada and represents Canadian population. First, I will introduce a visualisation technique that uses color to create a meaningful expectations from the results of a logistic regression. Then I will discuss the workflow of the project that implements this graphing system ( github.com/andkov/ipdln-2018-hackathon ). I will conclude by building the case to prefer reproducible workflows with version control over computational notebooks (e.g. Jupyter, R Notebook).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What Lies Beyond Acute Care Data</title>
      <link>/post/2019-08-01-what-lies-beyond-acute-care/</link>
      <pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-08-01-what-lies-beyond-acute-care/</guid>
      <description>


&lt;!-- # Publication type. --&gt;
&lt;!-- # Legend: --&gt;
&lt;!-- # 0 = Uncategorized --&gt;
&lt;!-- # 1 = Conference proceedings --&gt;
&lt;!-- # 2 = Journal --&gt;
&lt;!-- # 3 = Work in progress --&gt;
&lt;!-- # 4 = Technical report --&gt;
&lt;!-- # 5 = Book --&gt;
&lt;!-- # 6 = Book chapter --&gt;
&lt;p&gt;This report demonstrate what disappears from the view of service utilization analysts when only Emergency and Acute Care data are mined from the EHR, a common practice in health services research. We study the cohort of 4,067 residents of Vancouver Island with severe alcohol addiction. Engagement with the cross-continuum terrain of services is aggraged over 10 years (2007 - 2017) and reported via categories of the Clinical Context Coding Scheme of the Vancouver Island Health Authority.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://raw.githack.com/rbind/andriy/master/content/post/2019-08-01-what-lies-beyond-acute-care/what-lies-beyond.html&#34;&gt;Read the full report&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://raw.githack.com/rbind/andriy/master/content/post/2019-08-01-what-lies-beyond-acute-care/what-lies-beyond-pivot.html&#34;&gt;Play with the pivot of results&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;table1-cccs-summary.jpg&#34; /&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;terrain-0-service-class.png&#34; alt=&#34;Figure 1: Aggregate cohort engagement of service classes: acute (red) and community (grey)&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Figure 1: Aggregate cohort engagement of service classes: acute (red) and community (grey)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Academic Biography</title>
      <link>/post/academic-biography/</link>
      <pubDate>Thu, 20 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/academic-biography/</guid>
      <description>


&lt;p&gt;I am a data scientist with background in quantitative methods and interest in data-driven models of human aging.&lt;/p&gt;
&lt;p&gt;I received my Ph.D. in Quantitative Methods in 2014 from Vanderbilt University, where I studied under &lt;a href=&#34;https://www.vanderbilt.edu/psychological_sciences/bio/joe-rodgers&#34;&gt;Dr. Joe Rodgers&lt;/a&gt;, specializing in statistical modeling, design of graphical displays of information, and reproducible research. In August of the same year I joined the &lt;a href=&#34;https://www.maelstrom-research.org/mica/network/ialsa&#34;&gt;Integrative Analysis of Longitudinal Studies of Aging&lt;/a&gt; (IALSA) network (NIH/NIA P01AG043362) at the University of Victoria (UVic), directed by &lt;a href=&#34;http://www.uvic.ca/socialsciences/psychology/people/faculty-directory/piccininandrea.php&#34;&gt;Dr. Andrea Piccinin&lt;/a&gt; and &lt;a href=&#34;http://www.uvic.ca/socialsciences/psychology/people/faculty-directory/hoferscott.php&#34;&gt;Dr. Scott Hofer&lt;/a&gt;. With IALSA, I worked on developing &lt;a href=&#34;https://github.com/IALSA&#34;&gt;reproducible analytics&lt;/a&gt; (R + GitHub) for remote collaboration, particularly for implementing coordinated statistical analyses among multiple longitudinal studies of aging via remote participation.&lt;/p&gt;
&lt;p&gt;Interest in data-driven narratives of human aging lead me to explore the use of electronic health records (EHR) for research and improvement of patient care. Since October 2015 I have been working with Dr. Kenneth Moselle, the director of the &lt;a href=&#34;https://github.com/ihacru&#34;&gt;Applied Clinical Research Unit&lt;/a&gt; (ACRU) of Vancouver Island Health Authority (VIHA) on curating the transformation of VIHA’s EHR into analyzable form and creating opportunities for academic and clinical researchers to work with these data in responsible and reproducible way. Together with Dr. Moselle I have launched a Data Science Studio at the University of Victoria, a research unit at the &lt;a href=&#34;https://www.uvic.ca/research/centres/aging/&#34;&gt;Institute on Aging and Lifelong Health&lt;/a&gt; dedicated to supporting its research affiliates and UVic students in accessing, handling, and modeling cross-continuum health records of VIHA.&lt;/p&gt;
&lt;p&gt;At ACRU I worked on bridging healthcare data to analytic capacities of longitudinal modelling. My functions at ACRU included statistical and programming support for research and quality improvement projects, coordinating communication and empowering collaboration among three audiences: database managers, medical practitioners, and academic researchers. The overlap in my skills in statistical modeling, programming, and knowledge of VIHA’s electronic health records gave me a unique advantage to facilitate such a collaboration, the integrative nature of which offered great promises for improving patient care, medical science, and methodological practices of longitudinal research.&lt;/p&gt;
&lt;p&gt;In August of 2017 I was &lt;a href=&#34;http://www.newswire.ca/news-releases/minister-ginette-petitpas-taylor-announces-a-58-million-investment-in-programs-to-give-health-research-trainees-hands-on-work-experience-649094743.html&#34;&gt;awarded&lt;/a&gt; a &lt;a href=&#34;http://www.cihr-irsc.gc.ca/&#34;&gt;CIHR&lt;/a&gt; Health System Impact &lt;a href=&#34;http://www.cihr-irsc.gc.ca/e/50268.html&#34;&gt;Fellowship&lt;/a&gt; with BC &lt;a href=&#34;http://www.bccdc.ca/our-services/programs/bc-observatory-for-pop-public-health&#34;&gt;Observatory&lt;/a&gt; for Population and Public Health of the BC &lt;a href=&#34;http://www.bccdc.ca/&#34;&gt;Centre for Disease Control&lt;/a&gt; ($140,000 + $15,000 development fund). My program of work involved developing a system for population health surveillance that would focus on chronic diseases, with particular focus on mental health and substance use (MHSU) conditions, which tend to have high comorbidity rates, polysubstance use patterns, and slowly progressing pace of development. Drawing on my experience with EHR system employed by VIHA, I engaged various statistical modeling and learning techniques to construct analytic workflows capable of supporting clinical decisions at the point of service, while translating the acquired knowledge to be consumed by clinical stewards, system planners, and surveillance agencies.&lt;/p&gt;
&lt;div id=&#34;mindset&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;mindset&lt;/h2&gt;
&lt;p&gt;Data scientists describe the ultimate reality about data using various dialets of expression. Each translation has its benefits and disadvantages. We need them all to tell a good story.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;dialects/data-expression.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;No one language is better than the other. Each allows for different shades of distinction in model specification.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;dialects/model-expression.jpg&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Suppressing Small Counts for Public Release</title>
      <link>/publication/koval-2018-severity-burden-mental-health/</link>
      <pubDate>Wed, 29 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/koval-2018-severity-burden-mental-health/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Poster presented at the &lt;a href=&#34;https://www.cahspr.ca/en/conferences&#34;&gt;2019 conference of the Canadian Association for Health Services and Policy Research&lt;/a&gt;, Halifax, Nova Scotia.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;koval-2019-poster.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note: The number in parentheses &lt;em&gt;(1)&lt;/em&gt; refers to the section of the poster.&lt;/p&gt;
&lt;div id=&#34;take-away-points&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Take away points&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Reproducible pipelines are hard, but they pay off&lt;/li&gt;
&lt;li&gt;Think of cognitive load first, computational load second&lt;/li&gt;
&lt;li&gt;Invest into dependency maps for (re-)learning forms &amp;amp; functions&lt;/li&gt;
&lt;li&gt;Invest into workflow maps for (re-)learning structures &amp;amp; processes&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;In 2016, the Observatory for Population &amp;amp; Public Health of British Columbia launched the &lt;a href=&#34;http://www.bccdc.ca/health-info/disease-system-statistics/chronic-disease-dashboard&#34;&gt;Chronic Disease Dashboard&lt;/a&gt;, an online reporting tool designed to address the gap in surveillance of chronic diseases. To protect against re-identification risks, the Ministry of Health required redacting small counts prior to releasing disease rates into public domain. These preparations, when conducted manually, have proven to be arduous, time consuming, and prone to human error.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;approach&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Approach&lt;/h1&gt;
&lt;p&gt;While finding a “small” count (operationalized as “ &amp;lt; 5 ”) was straightforward, detecting conditions, in which suppressed values could be recalculated from related cells involved human judgement. As part of the embedded research by their &lt;a href=&#34;http://www.cihr-irsc.gc.ca/e/50660.html&#34;&gt;Health System Impact Fellow&lt;/a&gt; (2017), the Observatory set out to automate this task, designing a reproducible workflow ( see section 5 of the poster ) that split data into disease-by-year data frames of a specific form &lt;em&gt;( 1 )&lt;/em&gt;, applied a sequence of logical tests &lt;em&gt;( 2 )&lt;/em&gt; written to recognize conditions that made recalculation of suppressed values possible and printed a graph &lt;em&gt;( 6 )&lt;/em&gt; for each case of suggested automatic redaction to be confirmed by a human &lt;em&gt;( 7 )&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Results&lt;/h1&gt;
&lt;p&gt;The automated suppression was successfully integrated into the Dashboard maintenance. Data preparation, application of logical tests, and production of visual evidence were implemented in R and published as a version-controlled RStudio project on Github &lt;a href=&#34;https://github.com/IHACRU/suppress-for-release&#34;&gt;github.com/ihacru/suppress-for-release&lt;/a&gt;. A fully reproducible example with fictional data was made available to demonstrate the current logic of suppression and to ensure the availability of documentation for the future staff of the Observatory charged with Dashboard maintenance. To assist with learning, we designed a map of dependencies among the custom functions used for suppression &lt;em&gt;( 4 )&lt;/em&gt; and illustrated their use in a simplified workflow &lt;em&gt;( 3 )&lt;/em&gt;, which allows studying the performance of logical tests before engaging the real data to applying the suppression logic &lt;em&gt;( 5 )&lt;/em&gt; and to document the redaction decisions &lt;em&gt;( 7 )&lt;/em&gt; Anticipating the evolution of suppression logic, we moduralized the logical tests &lt;em&gt;( 2 )&lt;/em&gt; responsible for redaction and provided several options to vary the degree of preserved information ( blue vs light blue ).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;This case of embedded research demonstrates the benefits and feasibility of integrating practices of reproducible analytics into routine workflow of epidemiological surveillance. We make a strong case for employing such learning devices as workflow maps &lt;em&gt;( 5 )&lt;/em&gt; and function dependency trees &lt;em&gt;( 4 &lt;/em&gt;) for structuring applied projects and ensuring their reproducibility.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Suppressing Small Counts for Public Release</title>
      <link>/publication/koval-2019-suppress-for-release/</link>
      <pubDate>Wed, 29 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/koval-2019-suppress-for-release/</guid>
      <description>


&lt;p&gt;Poster presented at the &lt;a href=&#34;https://www.cahspr.ca/en/conferences&#34;&gt;2019 conference of the Canadian Association for Health Services and Policy Research&lt;/a&gt;, Halifax, Nova Scotia.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;koval-2019-poster.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note: The number in parentheses &lt;em&gt;(1)&lt;/em&gt; refers to the section of the poster.&lt;/p&gt;
&lt;div id=&#34;take-away-points&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Take away points&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Reproducible pipelines are hard, but they pay off&lt;/li&gt;
&lt;li&gt;Think of cognitive load first, computational load second&lt;/li&gt;
&lt;li&gt;Invest into dependency maps for (re-)learning forms &amp;amp; functions&lt;/li&gt;
&lt;li&gt;Invest into workflow maps for (re-)learning structures &amp;amp; processes&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;In 2016, the Observatory for Population &amp;amp; Public Health of British Columbia launched the &lt;a href=&#34;http://www.bccdc.ca/health-info/disease-system-statistics/chronic-disease-dashboard&#34;&gt;Chronic Disease Dashboard&lt;/a&gt;, an online reporting tool designed to address the gap in surveillance of chronic diseases. To protect against re-identification risks, the Ministry of Health required redacting small counts prior to releasing disease rates into public domain. These preparations, when conducted manually, have proven to be arduous, time consuming, and prone to human error.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;approach&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Approach&lt;/h1&gt;
&lt;p&gt;While finding a “small” count (operationalized as “ &amp;lt; 5 ”) was straightforward, detecting conditions, in which suppressed values could be recalculated from related cells involved human judgement. As part of the embedded research by their &lt;a href=&#34;http://www.cihr-irsc.gc.ca/e/50660.html&#34;&gt;Health System Impact Fellow&lt;/a&gt; (2017), the Observatory set out to automate this task, designing a reproducible workflow ( see section 5 of the poster ) that split data into disease-by-year data frames of a specific form &lt;em&gt;( 1 )&lt;/em&gt;, applied a sequence of logical tests &lt;em&gt;( 2 )&lt;/em&gt; written to recognize conditions that made recalculation of suppressed values possible and printed a graph &lt;em&gt;( 6 )&lt;/em&gt; for each case of suggested automatic redaction to be confirmed by a human &lt;em&gt;( 7 )&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Results&lt;/h1&gt;
&lt;p&gt;The automated suppression was successfully integrated into the Dashboard maintenance. Data preparation, application of logical tests, and production of visual evidence were implemented in R and published as a version-controlled RStudio project on Github &lt;a href=&#34;https://github.com/IHACRU/suppress-for-release&#34;&gt;github.com/ihacru/suppress-for-release&lt;/a&gt;. A fully reproducible example with fictional data was made available to demonstrate the current logic of suppression and to ensure the availability of documentation for the future staff of the Observatory charged with Dashboard maintenance. To assist with learning, we designed a map of dependencies among the custom functions used for suppression &lt;em&gt;( 4 )&lt;/em&gt; and illustrated their use in a simplified workflow &lt;em&gt;( 3 )&lt;/em&gt;, which allows studying the performance of logical tests before engaging the real data to applying the suppression logic &lt;em&gt;( 5 )&lt;/em&gt; and to document the redaction decisions &lt;em&gt;( 7 )&lt;/em&gt; Anticipating the evolution of suppression logic, we moduralized the logical tests &lt;em&gt;( 2 )&lt;/em&gt; responsible for redaction and provided several options to vary the degree of preserved information ( blue vs light blue ).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;This case of embedded research demonstrates the benefits and feasibility of integrating practices of reproducible analytics into routine workflow of epidemiological surveillance. We make a strong case for employing such learning devices as workflow maps &lt;em&gt;( 5 )&lt;/em&gt; and function dependency trees &lt;em&gt;( 4 &lt;/em&gt;) for structuring applied projects and ensuring their reproducibility.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Multi-Study Coordinated Meta-Analysis of Pulmonary Function and Cognition in Aging</title>
      <link>/publication/duggan-2018-pulmonary-cognition-aging/</link>
      <pubDate>Sat, 02 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/duggan-2018-pulmonary-cognition-aging/</guid>
      <description>


&lt;div id=&#34;background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;Substantial research is dedicated to understanding the aging-related dynamics among individual differences in level, change, and variation across physical and cognitive abilities. Evaluating replicability and synthesizing these findings has been limited by differences in measurements and samples, and by study design and statistical analyses confounding between-person differences with within-person changes. In this article, we conducted a coordinated analysis and summary meta-analysis of new results on the aging-related dynamics linking pulmonary function and cognitive performance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;methods&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Methods&lt;/h1&gt;
&lt;p&gt;We performed coordinated analysis of bivariate growth models in data from 20,586 participants across eight longitudinal studies to examine individual differences in baseline level, rate of change, and occasion-specific variability in pulmonary and cognitive functioning. Results were summarized using meta-analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Results&lt;/h1&gt;
&lt;p&gt;We found consistent but weak baseline and longitudinal associations in levels of pulmonary and cognitive functioning, but no associations in occasion-specific variability.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;Results provide limited evidence for a consistent link between simultaneous changes in pulmonary and cognitive function in a normal aging population. Further research is required to understand patterns of onset of decline and differences in rates of change within and across physical and cognitive functioning domains, both within-individuals and across countries and birth cohorts. Coordinated analysis provides an efficient and rigorous approach for replicating and comparing results across independent longitudinal studies.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/IALSA/ialsa-2017-portland/blob/master/libs/materials/pulmonary/README.md&#34;&gt;See more&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Figure-1-Mental-Status.jpg&#34; /&gt;
&lt;img src=&#34;Figure-2-Processing-Speed.jpg&#34; /&gt;
&lt;img src=&#34;Figure-3-Attention-and-Working-Memory.jpg&#34; /&gt;
&lt;img src=&#34;Figure-4-Perceptual-Reasoning.jpg&#34; /&gt;
&lt;img src=&#34;Figure-5-Verbal-Abilities.jpg&#34; /&gt;
&lt;img src=&#34;Figure-6-Learning-and-Memory.jpg&#34; /&gt;
&lt;img src=&#34;Figure-7-Total-Slope.jpg&#34; /&gt;
&lt;img src=&#34;Figure-8-ISR-Aggregate.jpg&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visualizing Logistic Regression</title>
      <link>/talk/2018-11-01-visualizing-logistic-regression/</link>
      <pubDate>Thu, 01 Nov 2018 16:00:00 -0700</pubDate>
      
      <guid>/talk/2018-11-01-visualizing-logistic-regression/</guid>
      <description>


&lt;p&gt;Visualising results of statistical modeling is a key component of data science workflow. Statistical graphs are often the best means to explain and promote research findings. However, in order to find that one graph that tells the story worth sharing, we sometimes have to try out and sift through many data visualizations. How should we approach such a task? What can we do to make it easier from both production and evaluation perspectives?&lt;/p&gt;
&lt;p&gt;This presentation will demonstrate a reproducible graphing system designed for the IPDLN-2018 hackathon. The system evaluates synthetic socioeconomic and mortality data with logistic regression. The data was prepared for the hackathon by Statistic Canada and represents Canadian population.&lt;/p&gt;
&lt;p&gt;Topics covered will include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Introduction to a visualisation technique that uses color to create meaningful expectations from the results of a logistic regression.&lt;/li&gt;
&lt;li&gt;Details related to the workflow of the project that implements this graphing system (github.com/andkov/ipdln-2018-hackathon )&lt;/li&gt;
&lt;li&gt;Building the case for preference of reproducible workflows with version control over computational notebooks (e.g. Jupyter, R Notebook).&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;video&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Video&lt;/h1&gt;
&lt;iframe width=&#34;680&#34; height=&#34;500&#34; src=&#34;https://www.youtube.com/embed/QtbhTu9HAzg&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>When notebooks are not enough</title>
      <link>/talk/2018-10-31-when-notebooks-are-not-enough/</link>
      <pubDate>Wed, 31 Oct 2018 15:00:00 -0700</pubDate>
      
      <guid>/talk/2018-10-31-when-notebooks-are-not-enough/</guid>
      <description>


&lt;div id=&#34;abstract&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Abstract&lt;/h1&gt;
&lt;p&gt;While computational notebooks offer scientists and engineers many helpful features, the limitations of this medium make it but a starting point in creating software - the practical goal of data science. Where do we go from computational notebooks if our projects require multiple interconnected scripts and dynamic documents? How do we ensure reproducibility amidst growing complexity of analyses and operations?&lt;/p&gt;
&lt;p&gt;I will use a concrete analytical example to demonstrate how constructing workflows for reproducible analyses can serve as the next step from computational notebooks towards creating an analytical software. First, I will demonstrate a reproducible graphing system designed for the IPDLN-2018 hackathon, organized by Statistics Canada. The system evaluates synthetic socioeconomic and mortality data with logistic regression. Then I will discuss the workflow of the project that implements this graphing system (github.com/andkov/ipdln-2018-hackathon ) and the RStudio + GitHub setup that hosts it. I will conclude by building the case to prefer reproducible workflows with version control over computational notebooks (e.g. Jupyter, R Notebook).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;table-of-content&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Table of Content&lt;/h1&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;figure1.jpg&#34; alt=&#34;Figure 1&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Figure 1&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;layers-of-isolation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Layers of Isolation&lt;/h1&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;figure2.jpg&#34; alt=&#34;Figure 2&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Figure 2&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Clinical Context Coding Scheme</title>
      <link>/publication/idpln-2018-banff-clinical-context-coding-scheme/</link>
      <pubDate>Sun, 23 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/idpln-2018-banff-clinical-context-coding-scheme/</guid>
      <description>


&lt;p&gt;Poster presented at the &lt;a href=&#34;https://www.cahspr.ca/en/conferences&#34;&gt;2018 conference of the International Population Data Linkage Association&lt;/a&gt;, Banff, Alberta.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;poster_Page_1.png&#34; /&gt;
&lt;img src=&#34;poster_Page_2.png&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      
      <guid>/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Independent and Interactive Impacts of Hypertension and Diabetes Mellitus on Verbal Memory</title>
      <link>/publication/kelly-2016-hypertension-diabetes-memory/</link>
      <pubDate>Thu, 25 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>/publication/kelly-2016-hypertension-diabetes-memory/</guid>
      <description>


&lt;p&gt;&lt;img src=&#34;figure2.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;figure3.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;figure1.png&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Toolbox and Toolsets of Reproducible Research</title>
      <link>/talk/2014-10-10-toolbox-toolset-reproducible-research/</link>
      <pubDate>Fri, 10 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/talk/2014-10-10-toolbox-toolset-reproducible-research/</guid>
      <description>


&lt;!-- # Publication type. --&gt;
&lt;!-- # Legend: --&gt;
&lt;!-- # 0 = Uncategorized --&gt;
&lt;!-- # 1 = Conference proceedings --&gt;
&lt;!-- # 2 = Journal --&gt;
&lt;!-- # 3 = Work in progress --&gt;
&lt;!-- # 4 = Technical report --&gt;
&lt;!-- # 5 = Book --&gt;
&lt;!-- # 6 = Book chapter --&gt;
&lt;p&gt;The lecture introduces reproducible research and demonstrates digital self-publishing with RStudio and Git (Hub). The skills described and emphasized in this workflow include data manipulation, graph production, statistical modeling, and dynamic reporting. A series of four talks discusses each skill and gives examples of possible implementations in R.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;traditional-vs-reproducible.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Of specific importance is the data grooming stage, which enables a productive exploration of the data and establishes custody chains to support research conclusions based on the analytic products.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;reproducible-data-prep.png&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
